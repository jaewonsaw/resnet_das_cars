{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11653969,"sourceType":"datasetVersion","datasetId":7313597}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(\"RUN CELL\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:53:47.629154Z","iopub.execute_input":"2025-05-03T21:53:47.629415Z","iopub.status.idle":"2025-05-03T21:53:53.421859Z","shell.execute_reply.started":"2025-05-03T21:53:47.629386Z","shell.execute_reply":"2025-05-03T21:53:53.421027Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/das-cnn-cars/data.pt\nRUN CELL\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, imgs, counts, labels, ids, transforms = lambda x: x):\n      self.imgs = imgs\n      self.labels = labels\n      self.counts = counts\n      self.labels = labels\n      self.transforms = transforms\n      self.ids = ids\n\n    def __getitem__(self, i, return_id = False):\n      if return_id:\n        return (self.transforms(self.imgs[i].unsqueeze(0)), self.labels[i], self.counts[i], self.ids[i])\n      return (self.transforms(self.imgs[i].unsqueeze(0)), self.labels[i], self.counts[i])\n\n    def __len__(self):\n      return len(self.imgs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:53:53.422729Z","iopub.execute_input":"2025-05-03T21:53:53.423142Z","iopub.status.idle":"2025-05-03T21:53:53.428575Z","shell.execute_reply.started":"2025-05-03T21:53:53.423116Z","shell.execute_reply":"2025-05-03T21:53:53.427814Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class SimpleVehicleNet(nn.Module):\n    def __init__(self, num_bins=8, num_count_classes = 7):  # num_bins = number of vehicle count buckets\n        super(SimpleVehicleNet, self).__init__()\n        self.num_count_classes = num_count_classes\n        # Input: [B, 1, 585, 130]\n        self.backbone = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # [B, 16, 585, 130]\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # [B, 16, 292, 65]\n\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # [B, 32, 292, 65]\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # [B, 32, 146, 32]\n\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # [B, 64, 146, 32]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),  # [B, 64, 1, 1]\n        )\n\n        self.flatten = nn.Flatten()  # → [B, 64]\n\n        # Regression head (predict scalar count)\n        self.count_head = nn.Sequential(\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_count_classes)\n        )\n\n        # Distribution head (predict vehicle count histogram)\n        self.dist_head = nn.Sequential(\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, num_bins)  # Output: raw logits → apply log_softmax\n        )\n        self.mse = nn.MSELoss()\n        self.mae = nn.L1Loss()\n        self.cross_entropy = nn.CrossEntropyLoss()\n        self.kl = nn.KLDivLoss(reduction = \"batchmean\")\n        self.lambda_count = 10\n        self.lambda_kl = 1\n        self.lambda_mse = 1\n        self.freq = torch.Tensor([3.8822e-02, 5.4998e-03, 8.6703e-01, 4.8528e-03, 0.0000e+00, 0.0000e+00,\n        3.2352e-04, 8.3468e-02])\n\n    def masked_mse(self, y_pred, y_true):\n        # Create a mask where target is non-zero\n        mask = (y_true != 0).float()\n        \n        # Compute squared error only where mask == 1\n        loss = (mask * (y_pred - y_true) ** 2)\n        \n        # Avoid division by zero: normalize by number of non-zero elements\n        return loss.sum() / (mask.sum() + 1e-8)\n\n    def weighted_mse(self, y_pred, y_true):\n        weights = 1.0 / (self.freq + 1e-6)\n        weights = weights / weights.sum()\n        weights = weights.to(y_pred.device)\n        loss = weights * (y_pred - y_true) ** 2\n        return loss.mean()\n    \n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.flatten(x)\n        count = self.count_head(x)                      # shape: [B, 1]\n        dist_logits = self.dist_head(x)                # shape: [B, num_bins]\n        dist_output = F.softmax(dist_logits, dim=1)\n        return count, dist_output\n\n    def loss(self, count_out, hist_out, count_label, hist_label):\n        label = F.one_hot(count_label, num_classes = self.num_count_classes).float()\n        #print(count_out.shape, label.shape)\n        #print(count_out.dtype, label.dtype)\n        count_loss = self.cross_entropy(count_out, label)\n        row_sums = hist_label.sum(dim=1, keepdim=True)\n        normalized = hist_label / row_sums\n        #hist_loss = self.weighted_mse(hist_out, hist_label)\n        hist_loss = self.cross_entropy(hist_out, normalized)\n        return self.lambda_count * count_loss + hist_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:53:53.430350Z","iopub.execute_input":"2025-05-03T21:53:53.430626Z","iopub.status.idle":"2025-05-03T21:53:53.449705Z","shell.execute_reply.started":"2025-05-03T21:53:53.430608Z","shell.execute_reply":"2025-05-03T21:53:53.448984Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class CountCNN(nn.Module):\n    def __init__(self, classes = 7):  # num_bins = number of vehicle count buckets\n        super(CountCNN, self).__init__()\n\n        # Input: [B, 1, 585, 130]\n        self.backbone = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # [B, 16, 585, 130]\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # [B, 16, 292, 65]\n\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # [B, 32, 292, 65]\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),  # [B, 32, 146, 32]\n\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # [B, 64, 146, 32]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),  # [B, 64, 1, 1]\n        )\n\n        self.flatten = nn.Flatten()  # → [B, 64]\n\n        # Regression head (predict scalar count)\n        self.count_head = nn.Sequential(\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, classes)\n        )\n        \n        self.mse = nn.MSELoss()\n        self.mae = nn.L1Loss()\n        self.cross_entropy = nn.CrossEntropyLoss()\n        self.freq = torch.Tensor([3.8822e-02, 5.4998e-03, 8.6703e-01, 4.8528e-03, 0.0000e+00, 0.0000e+00,\n        3.2352e-04, 8.3468e-02])\n        self.classes = classes\n        \n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.flatten(x)\n        count = self.count_head(x)                      # shape: [B, 1]\n        return count\n\n    def loss(self, count_out, count_label):\n        label = F.one_hot(count_label, num_classes = self.classes).float()\n        count_loss = self.cross_entropy(count_out, count_label)\n        return count_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:53:53.450556Z","iopub.execute_input":"2025-05-03T21:53:53.451395Z","iopub.status.idle":"2025-05-03T21:53:53.470881Z","shell.execute_reply.started":"2025-05-03T21:53:53.451353Z","shell.execute_reply":"2025-05-03T21:53:53.470071Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef append_dropout(model, rate=0.2):\n    for name, module in model.named_children():\n        if len(list(module.children())) > 0:\n            append_dropout(module)\n        if isinstance(module, nn.ReLU):\n            new = nn.Sequential(module, nn.Dropout2d(p=rate))\n            setattr(model, name, new)\n            \nclass VehicleCounterNet(nn.Module):\n    def __init__(self, num_classes=8, count_classes = 7):\n        super(VehicleCounterNet, self).__init__()\n\n        # Load pretrained ResNet18 and modify\n        self.backbone = models.resnet18()\n        #append_dropout(self.backbone, rate = 0.1)\n        # Modify input conv layer to take 1 channel (grayscale)\n        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\n        # Replace the final FC layer with identity so we can define our own heads\n        num_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Identity()\n\n        # Output head 1: Regress or classify number of vehicles\n        self.vehicle_count = nn.Sequential(nn.Linear(num_features, count_classes))\n\n        # Output head 2: Predict histogram (e.g., soft count distribution across possible values)\n        self.histogram_head = nn.Sequential(nn.Linear(num_features, num_classes))\n        self.cross_entropy = nn.CrossEntropyLoss()\n        self.freq = torch.Tensor([3.8822e-02, 5.4998e-03, 8.6703e-01, 4.8528e-03, 0.0000e+00, 0.0000e+00,\n        3.2352e-04, 8.3468e-02])\n        self.weighted_nll = nn.NLLLoss(weight = self.freq)\n        #lambda P, Q: -torch.sum(self.freq*P*torch.log(Q + 1e-9))\n        self.mse = nn.MSELoss()\n        self.kl = nn.KLDivLoss(reduction = \"batchmean\")\n        self.lambda_count = 10\n        self.lambda_kl = 1\n        self.lambda_mse = 1\n        self.count_classes = count_classes\n\n    def forward(self, x):\n        features = self.backbone(x)\n        count_output = self.vehicle_count(features)\n        histogram_output = F.softmax(self.histogram_head(features), dim = 1)\n        return count_output, histogram_output\n\n    def masked_mse(self, y_pred, y_true):\n        # Create a mask where target is non-zero\n        mask = (y_true != 0).float()\n        \n        # Compute squared error only where mask == 1\n        loss = (mask * (y_pred - y_true) ** 2)\n        \n        # Avoid division by zero: normalize by number of non-zero elements\n        return loss.sum() / (mask.sum() + 1e-8)\n\n    def weighted_soft_nll_loss(self, log_probs, soft_targets):\n        \"\"\"\n        log_probs: Tensor of shape [batch_size, num_classes], output of log_softmax\n        soft_targets: Tensor of shape [batch_size, num_classes], target distributions\n        class_weights: Tensor of shape [num_classes], weight per class\n        \"\"\"\n        # Expand class_weights to match soft_targets shape\n        weights = self.freq.unsqueeze(0).to(log_probs.device)  # [1, num_classes]\n        \n        # Apply weights to soft_targets\n        weighted_targets = soft_targets * weights  # [batch_size, num_classes]\n        \n        # Compute element-wise product: -weighted_targets * log_probs\n        loss = -torch.sum(weighted_targets * log_probs, dim=1)  # [batch_size]\n        \n        return loss.mean()  # or use .sum() if preferred\n\n\n\n    def loss(self, count_out, hist_out, count_label, hist_label):\n        count_label = F.one_hot(count_label, self.count_classes).float()\n        count_loss = self.cross_entropy(count_out, count_label)\n        target = hist_label + 1e-8\n        target_probs = (target) / target.sum(dim=1, keepdim=True)\n        expected_hist_loss = self.weighted_soft_nll_loss(torch.log(hist_out), target_probs)\n        total_loss = (\n          self.lambda_count * count_loss +\n          self.lambda_mse * expected_hist_loss\n        )\n        \n        return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:26:39.675173Z","iopub.execute_input":"2025-05-03T22:26:39.675496Z","iopub.status.idle":"2025-05-03T22:26:39.688029Z","shell.execute_reply.started":"2025-05-03T22:26:39.675470Z","shell.execute_reply":"2025-05-03T22:26:39.687213Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\nclass Logger:\n  def __init__(self, log_dir, filename='train_log.txt'):\n      os.makedirs(log_dir, exist_ok=True)\n      timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n      self.log_dir = log_dir\n      self.log_path = os.path.join(log_dir, f'{timestamp}_{filename}')\n\n      with open(self.log_path, 'w') as f:\n          f.write(f\"Logging started: {timestamp}\\n\\n\")\n\n  def log(self, message):\n      timestamp = datetime.now().strftime('%H:%M:%S')\n      full_message = f\"[{timestamp}] {message}\"\n      print(full_message)  # also print to stdout\n      with open(self.log_path, 'a') as f:\n          f.write(full_message + '\\n')\n\n  def log_metrics(self, epoch, train_loss=None, val_loss=None, **kwargs):\n      msg = f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\"\n      for k, v in kwargs.items():\n          msg += f\" | {k}: {v:.4f}\" if isinstance(v, float) else f\" | {k}: {v}\"\n      self.log(msg)\n\n  def save_checkpoint(self, path, model, optimizer, epoch, best_val_loss):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch,\n        'best_val_loss': best_val_loss\n    }\n    torch.save(checkpoint, os.path.join(self.log_dir, path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:53:58.629573Z","iopub.execute_input":"2025-05-03T21:53:58.629904Z","iopub.status.idle":"2025-05-03T21:53:58.637197Z","shell.execute_reply.started":"2025-05-03T21:53:58.629884Z","shell.execute_reply":"2025-05-03T21:53:58.636205Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\ndata = torch.load(\"/kaggle/input/das-cnn-cars/data.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:53:58.638131Z","iopub.execute_input":"2025-05-03T21:53:58.638737Z","iopub.status.idle":"2025-05-03T21:54:04.020251Z","shell.execute_reply.started":"2025-05-03T21:53:58.638707Z","shell.execute_reply":"2025-05-03T21:54:04.019692Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3726580803.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  data = torch.load(\"/kaggle/input/das-cnn-cars/data.pt\")\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:54:04.020951Z","iopub.execute_input":"2025-05-03T21:54:04.021141Z","iopub.status.idle":"2025-05-03T21:54:04.109138Z","shell.execute_reply.started":"2025-05-03T21:54:04.021125Z","shell.execute_reply":"2025-05-03T21:54:04.108236Z"}},"outputs":[{"name":"stdout","text":"{'imgs': tensor([[[-76.5721, -77.1576, -81.6660,  ..., -83.9075, -81.3769, -84.3168],\n         [-76.3438, -73.9983, -79.6962,  ..., -80.6483, -82.2012, -87.3538],\n         [-76.6467, -74.0830, -81.2737,  ..., -81.5403, -83.2910, -88.4794],\n         ...,\n         [-79.3313, -78.3320, -78.4678,  ..., -81.0519, -80.5077, -88.1365],\n         [-85.3054, -79.8374, -75.8968,  ..., -86.9083, -82.3489, -84.4151],\n         [-85.2439, -82.8430, -79.4188,  ..., -80.8857, -87.3668, -87.8926]],\n\n        [[-80.6762, -81.7623, -72.8934,  ..., -85.9792, -84.2322, -86.8569],\n         [-87.8347, -79.4662, -78.5318,  ..., -81.9379, -86.0406, -86.1646],\n         [-86.9451, -79.9672, -75.6647,  ..., -81.5951, -82.0449, -87.3756],\n         ...,\n         [-83.1090, -84.2905, -83.3142,  ..., -66.5070, -70.2364, -76.1732],\n         [-81.8832, -83.0903, -81.6652,  ..., -79.0737, -79.5598, -82.4076],\n         [-81.5252, -81.8138, -81.5861,  ..., -80.0465, -84.4012, -88.0107]],\n\n        [[-81.3736, -82.3884, -82.3694,  ..., -80.3928, -83.0735, -86.8679],\n         [-78.9946, -82.5016, -82.8398,  ..., -81.8434, -81.0418, -86.1020],\n         [-83.1371, -83.6057, -81.6349,  ..., -84.5285, -84.7080, -87.7729],\n         ...,\n         [-85.2765, -84.3534, -78.0785,  ..., -82.5390, -79.3208, -88.3852],\n         [-82.0490, -75.7326, -78.7181,  ..., -85.0579, -81.5047, -87.4834],\n         [-82.4501, -80.5147, -80.3095,  ..., -82.5188, -81.3438, -86.8624]],\n\n        ...,\n\n        [[-55.7491, -73.1584, -71.9997,  ..., -90.9453, -82.4147, -84.1545],\n         [-56.9756, -75.3451, -70.8708,  ..., -88.7276, -80.8550, -85.4529],\n         [-56.1515, -78.4077, -70.1182,  ..., -84.1965, -77.3435, -86.2985],\n         ...,\n         [-57.2164, -72.0829, -70.2730,  ..., -88.2212, -76.4385, -83.2914],\n         [-56.5730, -77.9725, -69.1155,  ..., -84.1077, -79.0128, -88.9237],\n         [-57.5560, -73.5970, -68.3237,  ..., -85.3207, -77.1153, -85.7009]],\n\n        [[-57.9705, -73.7127, -68.5833,  ..., -85.9930, -78.1848, -84.7927],\n         [-58.0371, -75.0461, -68.2246,  ..., -83.0186, -80.2691, -84.9181],\n         [-59.0240, -73.2208, -66.9845,  ..., -88.6362, -77.7781, -83.7724],\n         ...,\n         [-61.4826, -75.0030, -75.1707,  ..., -70.2614, -73.6829, -74.3008],\n         [-61.5806, -76.7696, -77.3115,  ..., -69.2116, -67.4479, -66.7784],\n         [-61.8567, -75.1457, -71.0938,  ..., -73.6981, -69.2790, -69.2046]],\n\n        [[-61.2920, -72.3046, -77.4041,  ..., -69.0234, -68.7917, -73.9711],\n         [-62.0409, -77.0577, -75.1115,  ..., -69.0184, -69.7108, -81.5067],\n         [-61.1606, -73.7213, -75.5934,  ..., -73.2688, -72.3475, -79.5513],\n         ...,\n         [-66.6696, -68.0272, -72.6041,  ..., -87.3704, -75.8738, -85.4036],\n         [-66.4949, -68.0423, -69.8421,  ..., -87.2891, -69.1672, -82.7263],\n         [-67.4131, -68.2040, -70.2057,  ..., -88.2045, -71.8227, -87.4893]]],\n       dtype=torch.float64), 'ids': tensor([   5,    6,    7,  ..., 2167, 2168, 2169]), 'counts': tensor([1, 1, 1,  ..., 5, 5, 5]), 'labels': tensor([[0, 0, 1,  ..., 0, 0, 0],\n        [0, 0, 1,  ..., 0, 0, 0],\n        [0, 0, 1,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 5,  ..., 0, 0, 0],\n        [0, 0, 5,  ..., 0, 0, 0],\n        [0, 0, 5,  ..., 0, 0, 0]])}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ncounts = data[\"counts\"]\nprint(torch.unique(counts, return_counts = True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:54:04.111207Z","iopub.execute_input":"2025-05-03T21:54:04.111644Z","iopub.status.idle":"2025-05-03T21:54:04.129669Z","shell.execute_reply.started":"2025-05-03T21:54:04.111625Z","shell.execute_reply":"2025-05-03T21:54:04.128806Z"}},"outputs":[{"name":"stdout","text":"(tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 17, 18, 19]), tensor([309, 111,  85,  77, 253, 122, 109,  85,  73, 100,  29,  13,  11,   3,\n          8,   1,   1,   1]))\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"filtered_data = {}\nfiltered_data[\"imgs\"] = []\nfiltered_data[\"counts\"] = []\nfiltered_data[\"labels\"] = []\nfiltered_data[\"ids\"] = []\n#filter out when counts <= 6\nfor i in range(len(counts)):\n    if counts[i] <= 6:\n        for k in [\"imgs\", \"counts\", \"labels\", \"ids\"]:\n            filtered_data[k].append(data[k][i].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:54:04.130335Z","iopub.execute_input":"2025-05-03T21:54:04.130637Z","iopub.status.idle":"2025-05-03T21:54:11.945463Z","shell.execute_reply.started":"2025-05-03T21:54:04.130621Z","shell.execute_reply":"2025-05-03T21:54:11.944825Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"for k in [\"imgs\", \"counts\", \"labels\", \"ids\"]:\n    filtered_data[k] = torch.tensor(filtered_data[k])\n    print(filtered_data[k].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:54:11.946228Z","iopub.execute_input":"2025-05-03T21:54:11.946501Z","iopub.status.idle":"2025-05-03T21:54:26.088093Z","shell.execute_reply.started":"2025-05-03T21:54:11.946474Z","shell.execute_reply":"2025-05-03T21:54:26.086754Z"}},"outputs":[{"name":"stdout","text":"torch.Size([957, 585, 153])\ntorch.Size([957])\ntorch.Size([957, 8])\ntorch.Size([957])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\nfrom torchvision.transforms import v2\nimages = filtered_data[\"imgs\"]\ncounts = filtered_data[\"counts\"]\nlabels = filtered_data[\"labels\"]\nids = filtered_data[\"ids\"]\ntrain_transform = v2.Compose([\n    v2.RandomHorizontalFlip(),         # flip image horizontally\n    v2.RandomRotation(15),             # rotate by ±15 degrees\n    v2.ToTensor(),                     # convert to tensor\n    v2.Normalize((0.5, ), (0.5, ))            # normalize\n])\ndataset = Dataset(images, counts, labels, ids, transforms = v2.Normalize((0.5, ), (0.5, )))\ntrain_ds, test_ds = random_split(dataset, [0.9, 0.1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:54:26.089089Z","iopub.execute_input":"2025-05-03T21:54:26.089481Z","iopub.status.idle":"2025-05-03T21:54:26.223761Z","shell.execute_reply.started":"2025-05-03T21:54:26.089449Z","shell.execute_reply":"2025-05-03T21:54:26.222851Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:54:26.224603Z","iopub.execute_input":"2025-05-03T21:54:26.224927Z","iopub.status.idle":"2025-05-03T21:54:26.228984Z","shell.execute_reply.started":"2025-05-03T21:54:26.224904Z","shell.execute_reply":"2025-05-03T21:54:26.228128Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model = VehicleCounterNet() #CountCNN()#VehicleCounterNet() #SimpleVehicleNet()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nlogger = Logger(\"/kaggle/working/182_proj_logs_cross_entropy/\")\ntrain_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\nval_loader = DataLoader(test_ds)\n# Loss and optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-2)\ntrain_losses = []\nval_losses = []\n# Training loop\nnum_epochs = 100\nbest_val_loss = float('inf')\nlogger.save_checkpoint(\"best_model.pth\", model, optimizer, 0, best_val_loss)\nfor epoch in tqdm(range(num_epochs)):\n    # --------- Train ---------\n    model.train()\n    train_loss = 0.0\n    train_count_correct = 0\n    train_label_correct = 0\n    train_total = 0\n\n    for images, labels, counts in train_loader:\n        images = images.to(device).float()\n        labels = labels.to(device).float()\n        counts = counts.to(device).long()\n        optimizer.zero_grad()\n        count_output, label_output = model(images)\n        loss = model.loss(count_output, label_output, counts, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        preds = torch.argmax(count_output, dim = -1)\n        label_preds = preds.unsqueeze(-1) * label_output\n        train_count_correct += (preds == counts).sum().item()\n        train_label_correct += (torch.round(label_preds).flatten() == labels.flatten()).sum().item()\n        train_total += labels.size(0)\n\n    avg_train_loss = train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    train_acc_count = train_count_correct / train_total\n    train_acc_label = train_label_correct / (8*train_total)\n\n    # --------- Validate ---------\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_label_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for images, labels, counts in val_loader:\n            images = images.to(device).float()\n            labels = labels.to(device).float()\n            counts = counts.to(device).long()#.unsqueeze(1)\n            count_output, label_output = model(images)\n            loss = model.loss(count_output, label_output, counts, labels)\n            val_loss += loss.item()\n            preds = torch.argmax(count_output, dim = -1)\n            label_preds = preds.unsqueeze(-1) * label_output\n            val_correct += (preds == counts.flatten()).sum().item()\n            val_label_correct += (torch.round(label_output).flatten() == labels.flatten()).sum().item()\n            val_total += labels.size(0)\n            \n    \n    avg_val_loss = val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n    val_acc = val_correct / val_total\n    val_acc_label = val_label_correct / (8*val_total)\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        logger.save_checkpoint(\"best_model.pth\", model, optimizer, epoch, best_val_loss)\n\n    # --------- Print results ---------\n    \n    logger.log(f\"Epoch {epoch+1}/{num_epochs} \"\n          f\"Train Loss: {avg_train_loss:.2f} | Count Acc: {train_acc_count:.2f} | Label Acc: {train_acc_label:.2f}\"\n          f\"|| Val Loss: {avg_val_loss:.2f} | Count Acc: {val_acc:.2f} | Label Acc: {val_acc_label:.2f}\")\n    '''\n    logger.log(f\"Epoch {epoch+1}/{num_epochs} \"\n          f\"Train Loss: {avg_train_loss:.2f} | Count Acc: {train_acc_count:.2f}\"\n          f\"|| Val Loss: {avg_val_loss:.2f} | Count Acc: {val_acc:.2f}\")   \n    '''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:44:52.860930Z","iopub.execute_input":"2025-05-03T22:44:52.861217Z","iopub.status.idle":"2025-05-03T22:54:20.249539Z","shell.execute_reply.started":"2025-05-03T22:44:52.861197Z","shell.execute_reply":"2025-05-03T22:54:20.248768Z"}},"outputs":[{"name":"stderr","text":"  1%|          | 1/100 [00:05<09:48,  5.95s/it]","output_type":"stream"},{"name":"stdout","text":"[22:44:59] Epoch 1/100 Train Loss: 18.64 | Count Acc: 0.38 | Label Acc: 0.88|| Val Loss: 19.60 | Count Acc: 0.34 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 2/100 [00:11<09:42,  5.95s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:05] Epoch 2/100 Train Loss: 15.37 | Count Acc: 0.44 | Label Acc: 0.90|| Val Loss: 16.64 | Count Acc: 0.37 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 3/100 [00:17<09:22,  5.80s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:10] Epoch 3/100 Train Loss: 14.86 | Count Acc: 0.46 | Label Acc: 0.90|| Val Loss: 16.94 | Count Acc: 0.38 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 4/100 [00:23<09:10,  5.73s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:16] Epoch 4/100 Train Loss: 14.63 | Count Acc: 0.48 | Label Acc: 0.90|| Val Loss: 25.82 | Count Acc: 0.31 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 5/100 [00:28<09:05,  5.74s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:22] Epoch 5/100 Train Loss: 14.22 | Count Acc: 0.48 | Label Acc: 0.90|| Val Loss: 53.32 | Count Acc: 0.20 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 6/100 [00:34<08:56,  5.71s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:27] Epoch 6/100 Train Loss: 14.24 | Count Acc: 0.49 | Label Acc: 0.90|| Val Loss: 16.88 | Count Acc: 0.39 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 7/100 [00:40<08:48,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:33] Epoch 7/100 Train Loss: 13.93 | Count Acc: 0.49 | Label Acc: 0.90|| Val Loss: 19.40 | Count Acc: 0.33 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 8/100 [00:46<08:50,  5.77s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:39] Epoch 8/100 Train Loss: 13.86 | Count Acc: 0.49 | Label Acc: 0.90|| Val Loss: 14.16 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 9/100 [00:51<08:40,  5.72s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:44] Epoch 9/100 Train Loss: 13.78 | Count Acc: 0.49 | Label Acc: 0.91|| Val Loss: 33.33 | Count Acc: 0.34 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 10/100 [00:57<08:31,  5.69s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:50] Epoch 10/100 Train Loss: 13.64 | Count Acc: 0.50 | Label Acc: 0.91|| Val Loss: 21.81 | Count Acc: 0.34 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 11/100 [01:03<08:25,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:45:56] Epoch 11/100 Train Loss: 13.24 | Count Acc: 0.52 | Label Acc: 0.90|| Val Loss: 25.22 | Count Acc: 0.34 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 12/100 [01:08<08:16,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:01] Epoch 12/100 Train Loss: 13.12 | Count Acc: 0.51 | Label Acc: 0.91|| Val Loss: 18.58 | Count Acc: 0.38 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 13/100 [01:14<08:10,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:07] Epoch 13/100 Train Loss: 12.99 | Count Acc: 0.50 | Label Acc: 0.91|| Val Loss: 15.03 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 14/100 [01:19<08:03,  5.62s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:12] Epoch 14/100 Train Loss: 12.89 | Count Acc: 0.53 | Label Acc: 0.91|| Val Loss: 14.32 | Count Acc: 0.51 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 15/100 [01:25<07:57,  5.61s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:18] Epoch 15/100 Train Loss: 12.88 | Count Acc: 0.53 | Label Acc: 0.91|| Val Loss: 20.84 | Count Acc: 0.37 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 16/100 [01:31<07:59,  5.70s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:24] Epoch 16/100 Train Loss: 13.10 | Count Acc: 0.51 | Label Acc: 0.91|| Val Loss: 13.31 | Count Acc: 0.52 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 17/100 [01:36<07:52,  5.70s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:30] Epoch 17/100 Train Loss: 12.66 | Count Acc: 0.52 | Label Acc: 0.91|| Val Loss: 40.39 | Count Acc: 0.17 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 18/100 [01:42<07:45,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:35] Epoch 18/100 Train Loss: 12.63 | Count Acc: 0.52 | Label Acc: 0.91|| Val Loss: 13.33 | Count Acc: 0.52 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 19/100 [01:48<07:38,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:41] Epoch 19/100 Train Loss: 12.67 | Count Acc: 0.53 | Label Acc: 0.91|| Val Loss: 13.80 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 20/100 [01:53<07:33,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:47] Epoch 20/100 Train Loss: 12.42 | Count Acc: 0.53 | Label Acc: 0.91|| Val Loss: 16.26 | Count Acc: 0.38 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 21/100 [01:59<07:27,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:52] Epoch 21/100 Train Loss: 12.26 | Count Acc: 0.54 | Label Acc: 0.91|| Val Loss: 15.11 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 22/100 [02:05<07:22,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:46:58] Epoch 22/100 Train Loss: 12.26 | Count Acc: 0.54 | Label Acc: 0.91|| Val Loss: 15.30 | Count Acc: 0.43 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 23/100 [02:11<07:24,  5.77s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:04] Epoch 23/100 Train Loss: 12.22 | Count Acc: 0.53 | Label Acc: 0.91|| Val Loss: 12.98 | Count Acc: 0.47 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 24/100 [02:16<07:15,  5.73s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:10] Epoch 24/100 Train Loss: 12.06 | Count Acc: 0.54 | Label Acc: 0.91|| Val Loss: 15.01 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 25/100 [02:22<07:07,  5.70s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:15] Epoch 25/100 Train Loss: 12.14 | Count Acc: 0.53 | Label Acc: 0.91|| Val Loss: 14.60 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 26/100 [02:28<07:00,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:21] Epoch 26/100 Train Loss: 12.08 | Count Acc: 0.53 | Label Acc: 0.91|| Val Loss: 13.48 | Count Acc: 0.49 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 27/100 [02:33<06:53,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:26] Epoch 27/100 Train Loss: 12.07 | Count Acc: 0.54 | Label Acc: 0.91|| Val Loss: 16.11 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 28/100 [02:39<06:48,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:32] Epoch 28/100 Train Loss: 11.58 | Count Acc: 0.56 | Label Acc: 0.91|| Val Loss: 14.31 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 29/100 [02:45<06:41,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:38] Epoch 29/100 Train Loss: 11.60 | Count Acc: 0.54 | Label Acc: 0.91|| Val Loss: 16.53 | Count Acc: 0.48 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 30/100 [02:50<06:36,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:43] Epoch 30/100 Train Loss: 11.62 | Count Acc: 0.55 | Label Acc: 0.91|| Val Loss: 15.20 | Count Acc: 0.41 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 31/100 [02:56<06:30,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:49] Epoch 31/100 Train Loss: 11.71 | Count Acc: 0.54 | Label Acc: 0.91|| Val Loss: 13.64 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 32/100 [03:02<06:23,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:47:55] Epoch 32/100 Train Loss: 11.40 | Count Acc: 0.56 | Label Acc: 0.91|| Val Loss: 31.67 | Count Acc: 0.35 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 33/100 [03:07<06:17,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:00] Epoch 33/100 Train Loss: 11.27 | Count Acc: 0.56 | Label Acc: 0.91|| Val Loss: 21.41 | Count Acc: 0.25 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 34/100 [03:13<06:12,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:06] Epoch 34/100 Train Loss: 11.25 | Count Acc: 0.56 | Label Acc: 0.91|| Val Loss: 16.24 | Count Acc: 0.44 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 35/100 [03:18<06:05,  5.63s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:12] Epoch 35/100 Train Loss: 11.20 | Count Acc: 0.55 | Label Acc: 0.91|| Val Loss: 72.24 | Count Acc: 0.17 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 36/100 [03:24<06:00,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:17] Epoch 36/100 Train Loss: 10.87 | Count Acc: 0.59 | Label Acc: 0.91|| Val Loss: 24.56 | Count Acc: 0.23 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 37/100 [03:30<05:54,  5.63s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:23] Epoch 37/100 Train Loss: 10.79 | Count Acc: 0.57 | Label Acc: 0.91|| Val Loss: 16.09 | Count Acc: 0.38 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 38/100 [03:35<05:49,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:29] Epoch 38/100 Train Loss: 10.75 | Count Acc: 0.57 | Label Acc: 0.91|| Val Loss: 18.18 | Count Acc: 0.43 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▉      | 39/100 [03:41<05:44,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:34] Epoch 39/100 Train Loss: 10.38 | Count Acc: 0.59 | Label Acc: 0.91|| Val Loss: 15.37 | Count Acc: 0.46 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 40/100 [03:47<05:38,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:40] Epoch 40/100 Train Loss: 10.37 | Count Acc: 0.59 | Label Acc: 0.91|| Val Loss: 16.71 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 41/100 [03:52<05:33,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:46] Epoch 41/100 Train Loss: 10.26 | Count Acc: 0.59 | Label Acc: 0.91|| Val Loss: 24.75 | Count Acc: 0.36 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 42/100 [03:58<05:28,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:51] Epoch 42/100 Train Loss: 9.98 | Count Acc: 0.61 | Label Acc: 0.91|| Val Loss: 19.56 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 43/100 [04:04<05:22,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:48:57] Epoch 43/100 Train Loss: 10.02 | Count Acc: 0.61 | Label Acc: 0.91|| Val Loss: 17.70 | Count Acc: 0.36 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 44/100 [04:09<05:17,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:03] Epoch 44/100 Train Loss: 9.58 | Count Acc: 0.64 | Label Acc: 0.91|| Val Loss: 28.04 | Count Acc: 0.26 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 45/100 [04:15<05:12,  5.69s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:08] Epoch 45/100 Train Loss: 9.42 | Count Acc: 0.64 | Label Acc: 0.91|| Val Loss: 18.97 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 46/100 [04:21<05:06,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:14] Epoch 46/100 Train Loss: 9.19 | Count Acc: 0.65 | Label Acc: 0.91|| Val Loss: 19.52 | Count Acc: 0.40 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 47/100 [04:26<05:00,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:20] Epoch 47/100 Train Loss: 8.86 | Count Acc: 0.66 | Label Acc: 0.91|| Val Loss: 15.84 | Count Acc: 0.47 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 48/100 [04:32<04:55,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:25] Epoch 48/100 Train Loss: 8.82 | Count Acc: 0.66 | Label Acc: 0.91|| Val Loss: 15.49 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 49/100 [04:38<04:49,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:31] Epoch 49/100 Train Loss: 8.22 | Count Acc: 0.70 | Label Acc: 0.91|| Val Loss: 16.39 | Count Acc: 0.41 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 50/100 [04:44<04:44,  5.70s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:37] Epoch 50/100 Train Loss: 7.93 | Count Acc: 0.71 | Label Acc: 0.91|| Val Loss: 15.25 | Count Acc: 0.51 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████     | 51/100 [04:49<04:39,  5.70s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:42] Epoch 51/100 Train Loss: 7.13 | Count Acc: 0.75 | Label Acc: 0.91|| Val Loss: 28.96 | Count Acc: 0.31 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 52/100 [04:55<04:33,  5.70s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:48] Epoch 52/100 Train Loss: 7.10 | Count Acc: 0.73 | Label Acc: 0.91|| Val Loss: 19.20 | Count Acc: 0.41 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 53/100 [05:01<04:27,  5.69s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:54] Epoch 53/100 Train Loss: 6.81 | Count Acc: 0.76 | Label Acc: 0.91|| Val Loss: 16.74 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 54/100 [05:06<04:20,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:49:59] Epoch 54/100 Train Loss: 6.56 | Count Acc: 0.76 | Label Acc: 0.92|| Val Loss: 19.38 | Count Acc: 0.47 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 55/100 [05:12<04:14,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:05] Epoch 55/100 Train Loss: 5.55 | Count Acc: 0.79 | Label Acc: 0.92|| Val Loss: 153.27 | Count Acc: 0.17 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▌    | 56/100 [05:18<04:08,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:11] Epoch 56/100 Train Loss: 5.78 | Count Acc: 0.79 | Label Acc: 0.91|| Val Loss: 20.24 | Count Acc: 0.51 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 57/100 [05:23<04:03,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:16] Epoch 57/100 Train Loss: 5.21 | Count Acc: 0.81 | Label Acc: 0.92|| Val Loss: 31.54 | Count Acc: 0.43 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 58/100 [05:29<03:57,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:22] Epoch 58/100 Train Loss: 5.18 | Count Acc: 0.82 | Label Acc: 0.91|| Val Loss: 24.07 | Count Acc: 0.38 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 59/100 [05:35<03:52,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:28] Epoch 59/100 Train Loss: 4.75 | Count Acc: 0.84 | Label Acc: 0.92|| Val Loss: 19.38 | Count Acc: 0.53 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 60/100 [05:40<03:47,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:33] Epoch 60/100 Train Loss: 3.83 | Count Acc: 0.86 | Label Acc: 0.92|| Val Loss: 23.26 | Count Acc: 0.49 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 61/100 [05:46<03:41,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:39] Epoch 61/100 Train Loss: 3.43 | Count Acc: 0.88 | Label Acc: 0.92|| Val Loss: 123.06 | Count Acc: 0.19 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 62/100 [05:52<03:35,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:45] Epoch 62/100 Train Loss: 3.28 | Count Acc: 0.89 | Label Acc: 0.92|| Val Loss: 26.80 | Count Acc: 0.39 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 63/100 [05:57<03:29,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:50] Epoch 63/100 Train Loss: 3.63 | Count Acc: 0.87 | Label Acc: 0.92|| Val Loss: 26.89 | Count Acc: 0.46 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 64/100 [06:03<03:23,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:50:56] Epoch 64/100 Train Loss: 2.74 | Count Acc: 0.91 | Label Acc: 0.92|| Val Loss: 24.17 | Count Acc: 0.52 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 65/100 [06:08<03:17,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:02] Epoch 65/100 Train Loss: 2.99 | Count Acc: 0.89 | Label Acc: 0.92|| Val Loss: 25.59 | Count Acc: 0.54 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 66/100 [06:14<03:11,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:07] Epoch 66/100 Train Loss: 2.09 | Count Acc: 0.92 | Label Acc: 0.92|| Val Loss: 55.71 | Count Acc: 0.38 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 67/100 [06:20<03:06,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:13] Epoch 67/100 Train Loss: 2.29 | Count Acc: 0.93 | Label Acc: 0.92|| Val Loss: 24.35 | Count Acc: 0.40 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 68/100 [06:25<03:01,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:19] Epoch 68/100 Train Loss: 2.00 | Count Acc: 0.93 | Label Acc: 0.92|| Val Loss: 34.10 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 69/100 [06:31<02:55,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:24] Epoch 69/100 Train Loss: 1.88 | Count Acc: 0.93 | Label Acc: 0.92|| Val Loss: 29.29 | Count Acc: 0.41 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 70/100 [06:37<02:49,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:30] Epoch 70/100 Train Loss: 2.13 | Count Acc: 0.93 | Label Acc: 0.92|| Val Loss: 33.18 | Count Acc: 0.44 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████   | 71/100 [06:42<02:44,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:36] Epoch 71/100 Train Loss: 2.33 | Count Acc: 0.93 | Label Acc: 0.92|| Val Loss: 25.85 | Count Acc: 0.38 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 72/100 [06:48<02:38,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:41] Epoch 72/100 Train Loss: 1.54 | Count Acc: 0.95 | Label Acc: 0.92|| Val Loss: 58.41 | Count Acc: 0.38 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 73/100 [06:54<02:33,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:47] Epoch 73/100 Train Loss: 1.48 | Count Acc: 0.95 | Label Acc: 0.92|| Val Loss: 39.08 | Count Acc: 0.41 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 74/100 [06:59<02:27,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:53] Epoch 74/100 Train Loss: 1.07 | Count Acc: 0.97 | Label Acc: 0.93|| Val Loss: 30.20 | Count Acc: 0.43 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 75/100 [07:05<02:21,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:51:58] Epoch 75/100 Train Loss: 0.86 | Count Acc: 0.98 | Label Acc: 0.93|| Val Loss: 34.57 | Count Acc: 0.40 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 76/100 [07:11<02:15,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:04] Epoch 76/100 Train Loss: 1.83 | Count Acc: 0.94 | Label Acc: 0.92|| Val Loss: 30.93 | Count Acc: 0.48 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 77/100 [07:16<02:09,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:10] Epoch 77/100 Train Loss: 1.09 | Count Acc: 0.97 | Label Acc: 0.93|| Val Loss: 27.13 | Count Acc: 0.48 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 78/100 [07:22<02:04,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:15] Epoch 78/100 Train Loss: 0.88 | Count Acc: 0.98 | Label Acc: 0.93|| Val Loss: 43.75 | Count Acc: 0.46 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 79/100 [07:28<01:59,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:21] Epoch 79/100 Train Loss: 1.26 | Count Acc: 0.97 | Label Acc: 0.93|| Val Loss: 31.91 | Count Acc: 0.46 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 80/100 [07:33<01:53,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:27] Epoch 80/100 Train Loss: 1.75 | Count Acc: 0.94 | Label Acc: 0.92|| Val Loss: 32.82 | Count Acc: 0.44 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 81/100 [07:39<01:47,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:32] Epoch 81/100 Train Loss: 2.04 | Count Acc: 0.93 | Label Acc: 0.92|| Val Loss: 96.27 | Count Acc: 0.19 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 82/100 [07:45<01:41,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:38] Epoch 82/100 Train Loss: 0.91 | Count Acc: 0.97 | Label Acc: 0.93|| Val Loss: 39.97 | Count Acc: 0.43 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 83/100 [07:50<01:35,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:43] Epoch 83/100 Train Loss: 0.54 | Count Acc: 0.99 | Label Acc: 0.93|| Val Loss: 40.81 | Count Acc: 0.43 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 84/100 [07:56<01:30,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:49] Epoch 84/100 Train Loss: 0.22 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 34.83 | Count Acc: 0.47 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 85/100 [08:02<01:24,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:52:55] Epoch 85/100 Train Loss: 0.49 | Count Acc: 0.99 | Label Acc: 0.93|| Val Loss: 34.35 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 86/100 [08:07<01:19,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:00] Epoch 86/100 Train Loss: 0.97 | Count Acc: 0.98 | Label Acc: 0.93|| Val Loss: 30.47 | Count Acc: 0.46 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 87/100 [08:13<01:13,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:06] Epoch 87/100 Train Loss: 2.40 | Count Acc: 0.93 | Label Acc: 0.92|| Val Loss: 30.55 | Count Acc: 0.45 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 88/100 [08:19<01:07,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:12] Epoch 88/100 Train Loss: 2.13 | Count Acc: 0.93 | Label Acc: 0.92|| Val Loss: 42.60 | Count Acc: 0.40 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 89/100 [08:24<01:02,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:17] Epoch 89/100 Train Loss: 0.53 | Count Acc: 0.99 | Label Acc: 0.93|| Val Loss: 30.61 | Count Acc: 0.49 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 90/100 [08:30<00:56,  5.68s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:23] Epoch 90/100 Train Loss: 0.21 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 33.61 | Count Acc: 0.49 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████ | 91/100 [08:36<00:51,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:29] Epoch 91/100 Train Loss: 0.20 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 37.34 | Count Acc: 0.48 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 92/100 [08:41<00:45,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:34] Epoch 92/100 Train Loss: 0.11 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 35.52 | Count Acc: 0.53 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 93/100 [08:47<00:39,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:40] Epoch 93/100 Train Loss: 0.07 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 36.07 | Count Acc: 0.53 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 94/100 [08:53<00:33,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:46] Epoch 94/100 Train Loss: 0.06 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 37.58 | Count Acc: 0.53 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 95/100 [08:58<00:28,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:51] Epoch 95/100 Train Loss: 0.06 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 35.65 | Count Acc: 0.53 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 96/100 [09:04<00:22,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:53:57] Epoch 96/100 Train Loss: 0.05 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 37.53 | Count Acc: 0.52 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 97/100 [09:10<00:16,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:54:03] Epoch 97/100 Train Loss: 0.05 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 39.65 | Count Acc: 0.49 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 98/100 [09:15<00:11,  5.66s/it]","output_type":"stream"},{"name":"stdout","text":"[22:54:08] Epoch 98/100 Train Loss: 0.07 | Count Acc: 1.00 | Label Acc: 0.93|| Val Loss: 47.53 | Count Acc: 0.43 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [09:21<00:05,  5.65s/it]","output_type":"stream"},{"name":"stdout","text":"[22:54:14] Epoch 99/100 Train Loss: 7.69 | Count Acc: 0.81 | Label Acc: 0.91|| Val Loss: 22.08 | Count Acc: 0.39 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [09:27<00:00,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[22:54:20] Epoch 100/100 Train Loss: 1.10 | Count Acc: 0.97 | Label Acc: 0.93|| Val Loss: 35.72 | Count Acc: 0.42 | Label Acc: 0.88\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":" with torch.no_grad():\n    for images, labels, counts in val_loader:\n        images = images.to(device).float()\n        labels = labels.to(device).float()\n        counts = counts.to(device).float().unsqueeze(1)\n        count_output, label_output = model(images)\n        loss = model.loss(count_output, label_output, counts, labels)\n        val_loss += loss.item()\n        preds = torch.round(count_output).flatten()\n        val_correct += (preds == counts.flatten()).sum().item()\n        val_label_correct += (torch.round(label_output).flatten() == labels.flatten()).sum().item()\n        val_total += labels.size(0)\n        print(label_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:04:19.004855Z","iopub.execute_input":"2025-05-03T22:04:19.005139Z","iopub.status.idle":"2025-05-03T22:04:19.061623Z","shell.execute_reply.started":"2025-05-03T22:04:19.005113Z","shell.execute_reply":"2025-05-03T22:04:19.060680Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2004978071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m        \u001b[0mcount_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m        \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m        \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1673388985.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, count_out, hist_out, count_label, hist_label)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mcount_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mcount_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist_label\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor of type LongTensor."],"ename":"RuntimeError","evalue":"one_hot is only applicable to index tensor of type LongTensor.","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"freq = filtered_data[\"labels\"].sum(dim = 0)\nfreq = freq / freq.sum()\nprint(freq)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T22:04:19.062433Z","iopub.status.idle":"2025-05-03T22:04:19.062819Z","shell.execute_reply.started":"2025-05-03T22:04:19.062620Z","shell.execute_reply":"2025-05-03T22:04:19.062637Z"}},"outputs":[],"execution_count":null}]}